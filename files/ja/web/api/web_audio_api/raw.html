<div class="summary">
<p><span class="seoSummary"><strong>Web Audio APIはWeb上で音声を扱うための強力で多機能なシステムを提供します。これにより開発者はオーディオソースを選択したり、エフェクトを加えたり、ビジュアライゼーションを加えたり、パンニングなどの特殊効果を適用したり、他にもたくさんのいろいろなことができるようになります。</strong></span></p>
</div>

<h2 id="Web_audio_の概念と利用方法">Web audio の概念と利用方法</h2>

<p>Web Audio API は音声操作を<strong>オーディオコンテキスト</strong>内の操作として実現し、モジュラールーティングできるようにデザインされています。基本的な操作は <strong>オーディオノード</strong>として表現されています。これを接続することで、オーディオグラフを作成します。 チャンネル構成の異なる複数の音源も 1 つのコンテキスト内で扱えます。この構成によって、複雑で動的な音声操作を実現できるようになっています。</p>

<p>Audio nodes are linked into chains and simple webs by their inputs and outputs. They typically start with one or more sources. ソースはごく短時間に(毎秒数万程度)音の強度の配列を提供します。 ソースは{{domxref("OscillatorNode")}} などで数学的に計算されるか、 {{domxref("AudioBufferSourceNode")}} や{{domxref("MediaElementAudioSourceNode")}} のような音声/動画ファイルや、 オーディオストリーム({{domxref("MediaStreamAudioSourceNode")}}) から来ます。実際、音声ファイルは要するにマイクであるとか電子的に生成された音の強度の記録なのであり、最終的に単一の複雑な波へとミックスされるわけです。</p>

<p>ノードの出力は他のノードの入力に紐付けられます。つまり、入力ストリームにミックスや編集をして他へ出力できるわけです。一般的な編集の例としては音量の変更です( {{domxref("GainNode")}} )。意図した効果を十分に施したあと、ユーザーに音声を聞かせたい場合、サウンドをスピーカーやヘッドオンに流すために、 {{domxref("AudioContext.destination")}} の入力に紐付ける必要があります。</p>

<p>簡潔で通常のWeb Audioの使い方は次のようになります:</p>

<ol>
 <li>オーディオコンテキストを作成する</li>
 <li>コンテキストの中で、<code>&lt;audio&gt;</code>,オシレーター,ストリームなどのソースを作成する</li>
 <li>リバーブ・フィルター・パンナー・コンプレッサーなどのエフェクトノードを作成する</li>
 <li>最終的な音声の到達先を選ぶ(例えばスピーカー)</li>
 <li>ソースをエフェクトに繋げ、エフェクトを到達先(destination)に繋げる</li>
</ol>

<p><img alt="A simple box diagram with an outer box labeled Audio context, and three inner boxes labeled Sources, Effects and Destination. The three inner boxes have arrow between them pointing from left to right, indicating the flow of audio information." src="https://mdn.mozillademos.org/files/7893/web-audio-api-flowchart.png" style="display: block; height: 113px; margin: 0px auto; width: 635px;"></p>

<p>タイミングは高精度で低遅延に制御されます。正確にイベントに反応したり特定の音声サンプルにアクセスしたりすることができます。ドラムマシンやシーケンサーのようなアプリケーションを作ることができます。</p>

<p>Web Audio API は立体音響も扱えます。 <em>source-listener モデル </em>に基づいたシステムを利用し<em>、パンニングモデル</em>をコントロールできます。また距離に基づく音の減衰や、ドップラー効果(ソースや観測者の移動)も扱えます。</p>

<div class="note">
<p><strong>付記</strong>: Web Audio API の理論に関する詳細は <a href="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Basic concepts behind Web Audio API</a> をご覧ください。</p>
</div>

<h2 id="Web_Audio_APIがターゲットとする人">Web Audio APIがターゲットとする人</h2>

<p>Web Audio APIは音声技術に馴染みがない人にとって、怖気づくかもしれません。また、多くの機能があるため、開発者にとってとっつきにくいものになっています。</p>

<p>Web Audio APIの用途としては、<a href="https://www.futurelibrary.no/">futurelibrary.no</a>のような雰囲気作りのためや<a href="https://css-tricks.com/form-validation-web-audio/">フォームの検証に音を活用</a>するために、単に音声をWebサイトに組み込むために使用できます。一方で、高度な対話型ツールの作成にも利用できます。こうしたことを踏まえると、開発者とミュージシャンの双方に適していると言えます。</p>

<p>プログラミングは得意だけどAPIの構造と用語の解説が必要な人のために、<a href="/ja/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">簡単なチュートリアル</a>があります。</p>

<p>また、<a href="/ja/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Web Audio APIの背景にある基本理念</a>に関する記事もあり、特にこのAPIの範囲でデジタルオーディオがどのように動作するのかを理解するのに役立ちます。またAPIの基礎となる優れた概念の紹介も含んでいます。</p>

<p>プログラムを書く作業はカードゲームに例えられます。ルールを学んで遊んでみて、またルールを学んで再び遊んでみて・・・。したがって最初のチュートリアルと記事を見たあとにピンとこなかった場合、最初のチュートリアルを拡張して、学んだことを実践して、段階的に高度なことを学んでいく<a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques">発展的なチュートリアル</a>があります。</p>

<p>それとは別に、このAPIのすべての機能を網羅したチュートリアルとリファレンスを用意しています。このページのサイドバーを参照して下さい。</p>

<p><span class="tlid-translation translation" lang="ja">音楽的な側面に精通し、音楽理論の概念に精通し、楽器の構築を開始したい場合は</span>、発展的チュートリアルやその他の解説を基に制作に移る事ができるでしょう(上記のリンク付きチュートリアルでは、スケジューリングに関する注意事項、オーダーメイドの発振器やエンベロープの作成、LFOなどについて説明しています) 。</p>

<p>プログラミングの基本に慣れていない場合は、まず初級者向けのJavaScriptチュートリアルを参照してから、このページに戻ってください。<a href="/ja/docs/Learn/JavaScript">初級者向けのJavaScript学習モジュール</a>を参照してください。</p>

<h2 id="Web_Audio_API_インタフェース">Web Audio API インタフェース</h2>

<p>Web Audio API は全部で28のインタフェースと関連するイベントを持ちます。それらは機能的に9個のカテゴリに分けられます。</p>

<h3 id="一般的なオーディオグラフの定義">一般的なオーディオグラフの定義</h3>

<p>Web Audio API で利用するオーディオグラフのコンテナと、その構成要素は以下の通りです。</p>

<dl>
 <dt>{{domxref("AudioContext")}}</dt>
 <dd>音声モジュールを組み合わせて作成される、音声処理のグラフを表します。グラフ内の各モジュールは {{domxref("AudioNode")}} として表現されています。オーディオコンテキストは、コンテキスト内での処理を担当するノードの作成を行います。</dd>
 <dt>{{domxref("AudioContextOptions")}}</dt>
 <dd><code><strong>AudioContextOptions</strong></code> は <code>AudioContext</code> を作成するときにオプションを渡したいときに使用します。</dd>
 <dt>{{domxref("AudioNode")}}</dt>
 <dd><strong><code>AudioNode</code></strong><strong> </strong>インタフェースは音声処理のモジュールの表現しています。これには{{HTMLElement("audio")}}要素や{{HTMLElement("video")}} 要素のような音源、音声の出力先、{{domxref("BiquadFilterNode")}} や {{domxref("GainNode")}}) のようなフィルタなどが含まれます。</dd>
 <dt>{{domxref("AudioParam")}}</dt>
 <dd><strong><code>AudioParam</code></strong><strong> </strong>インタフェースは {{domxref("AudioNode")}} の持つような、音声に関するパラメータを表現しています。値をセットするだけでなく、差分を指定することも可能です。また指定した時間やパターンで、値を変更をすることもできます。</dd>
 <dt>{{domxref("AudioParamMap")}}</dt>
 <dd>{{domxref("AudioParam")}} のマップのようなインターフェイスを提供します。つまり <code>forEach()</code>、<code>get()</code>、 <code>has()</code>、 <code>keys()</code>、 <code>values()</code>メソッドや <code>size</code> プロパティが使えます。</dd>
 <dt>{{domxref("BaseAudioContext")}}</dt>
 <dd><strong><code>BaseAudioContext</code></strong> インターフェイスはオンライン音声処理グラフ( {{domxref("AudioContext")}})やオフライン音声処理グラフ( {{domxref("OfflineAudioContext")}})の基底となるものです。直接 <code>BaseAudioContext</code> を使うことはなく、これを継承するこれら2つのインターフェイスを介して使用します。</dd>
 <dt>{{event("ended_(Web_Audio)", "ended")}} (event)</dt>
 <dd><code>ended</code> イベントは、再生が終了した際に発火するイベントです。</dd>
</dl>

<h3 id="音源">音源</h3>

<p>Web Audio API 内で利用できる音源は以下の通りです。</p>

<dl>
 <dt>{{domxref("AudioScheduledSourceNode")}}</dt>
 <dd><strong><code>AudioScheduledSourceNode</code></strong> は {{domxref("AudioNode")}} の一種で、いくつかの音源ノードの親インターフェイスです。</dd>
 <dt>{{domxref("OscillatorNode")}}</dt>
 <dd><strong><code style="font-size: 14px;">OscillatorNode</code></strong><strong> </strong>はサイン波を出力する {{domxref("AudioNode")}} です。出力する波形の周波数を指定できます。</dd>
 <dt>{{domxref("AudioBuffer")}}</dt>
 <dd> <strong><code>AudioBuffer</code></strong> はメモリ上に展開された短い音声データを表します。 {{ domxref("AudioContext.createBuffer()") }} によって音声ファイルから、もしくは{{ domxref("AudioContext.createBuffer()") }} メソッドによって生データから作成されます。このデータは {{ domxref("AudioBufferSourceNode") }} を利用して再生されます。</dd>
 <dt>{{domxref("AudioBufferSourceNode")}}</dt>
 <dd><strong><code>AudioBufferSourceNode</code></strong> は {{domxref("AudioNode")}} の一種で、メモリ上の音声データを利用した音源です。音声データ自身は {{domxref("AudioBuffer")}} として保存されています。</dd>
 <dt>{{domxref("MediaElementAudioSourceNode")}}</dt>
 <dd><code><strong>MediaElementAudio</strong></code><strong><code>SourceNode</code></strong> は {{domxref("AudioNode")}} の一種で、{{ htmlelement("audio") }} 要素や {{ htmlelement("video") }} 要素を利用する音源です。</dd>
 <dt>{{domxref("MediaStreamAudioSourceNode")}}</dt>
 <dd><code><strong>MediaStreamAudio</strong></code><strong><code>SourceNode</code></strong> は {{domxref("AudioNode")}} の一種で、マイクやWebカメラといった <a href="/en-US/docs/WebRTC" title="/en-US/docs/WebRTC">WebRTC</a> {{domxref("MediaStream")}} からの入力を扱える音源です。複数の音声トラックがストリーム上にある場合、 {{domxref("MediaStreamTrack.id", "id")}} は辞書順(アルファベット順)です。</dd>
 <dt>{{domxref("MediaStreamTrackAudioSourceNode")}}</dt>
 <dd><code>MediaStreamTrackAudioSourceNode</code>は{{domxref("AudioNode")}} の一種で、 {{domxref("MediaStreamTrack")}} からの入力を扱える音源です。 {{domxref("AudioContext.createMediaStreamTrackSource", "createMediaStreamTrackSource()")}} メソッドによって作られたノードの場合、使用するトラックを指定してください。 <code>MediaStreamAudioSourceNode</code> 以上の制御を提供します。</dd>
</dl>

<h3 id="オーディオエフェクトフィルター">オーディオエフェクトフィルター</h3>

<p>これらを利用すると、音源からの音声にエフェクトをかけられます。</p>

<dl>
 <dt>{{domxref("BiquadFilterNode")}}</dt>
 <dd><strong><code>BiquadFilterNode</code></strong><strong> </strong>は {{domxref("AudioNode")}} の一種で、単純な低次フィルタです。フィルタやトーンコントロール、グラフィックイコライザで利用されます。<code>BiquadFilterNode</code> の入力と出力はともに 1 つです。</dd>
 <dt>{{domxref("ConvolverNode")}}</dt>
 <dd><code><strong>Convolver</strong></code><strong><code>Node</code></strong><strong> </strong>は<span style="line-height: 1.5;">{{domxref("AudioNode")}} の一種で、Audiobuffer に対して線形畳み込みを行います。リバーブの実現に利用されます。</span></dd>
 <dt>{{domxref("DelayNode")}}</dt>
 <dd><strong><code>DelayNode</code></strong><strong> </strong>は {{domxref("AudioNode")}} の一種で、<a href="http://en.wikipedia.org/wiki/Digital_delay_line" title="http://en.wikipedia.org/wiki/Digital_delay_line">delay-line</a> を表します。入力された音声を、遅らせて出力します。</dd>
 <dt>{{domxref("DynamicsCompressorNode")}}</dt>
 <dd><strong><code>DynamicsCompressorNode</code></strong> はコンプレッサとして働きます。大きな音の音量を絞ることで、複数の音を同時に再生した時に起きがちな、音のクリッピングや歪みを回避します。</dd>
 <dt>{{domxref("GainNode")}}</dt>
 <dd><strong><code>GainNode</code></strong><strong> </strong>は {{domxref("AudioNode")}} の一種で、入力された音の音量を指定されたものに変更して出力します。</dd>
 <dt>{{domxref("WaveShaperNode")}}</dt>
 <dd><strong><code>WaveShaperNode</code></strong><strong> </strong>は {{domxref("AudioNode")}} の一種で、非線形のディストーションエフェクトを実現します。curve 属性に指定された関数を用いて、入力を歪ませます。音を歪ませ、温かみを与えるために用いられます。</dd>
 <dt>{{domxref("PeriodicWaveNode")}}</dt>
 <dd>{{ domxref("OscillatorNode") }} の出力の波形を変えるために用いられます。</dd>
 <dt>{{domxref("IIRFilterNode")}}</dt>
 <dd>一般的な<a href="https://ja.wikipedia.org/wiki/%E7%84%A1%E9%99%90%E3%82%A4%E3%83%B3%E3%83%91%E3%83%AB%E3%82%B9%E5%BF%9C%E7%AD%94">無限インパルス応答(IIR)</a>フィルターの実装です。トーン制御デバイスやグラフィックイコライザーの実装に利用できます。</dd>
</dl>

<h3 id="音声の出力先">音声の出力先</h3>

<p>処理した音声の出力先を、以下のもので定めます。</p>

<dl>
 <dt>{{domxref("AudioDestinationNode")}}</dt>
 <dd><strong><code>AudioDestinationNode</code></strong> はコンテキスト内での出力先を表します。通常はスピーカとなっています。</dd>
 <dt>{{domxref("MediaStreamAudioDestinationNode")}}</dt>
 <dd><code><strong>MediaElementAudio</strong></code><strong><code>SourceNode</code></strong> は音声の出力先となる {{domxref("AudioNode")}} の一種で、<a href="/en-US/docs/WebRTC" title="/en-US/docs/WebRTC">WebRTC</a> {{domxref("MediaStream")}} と1 つの <code>AudioMediaStreamTrack</code> から構成されます。{{ domxref("Navigator.getUserMedia") }} で取得された MediaStream と同様に扱えます。</dd>
</dl>

<h3 id="分析と可視化">分析と可視化</h3>

<p>音声の時間領域 / 周波数領域分析には、<code>AnalyserNode</code> を利用します。</p>

<dl>
 <dt>{{domxref("AnalyserNode")}}</dt>
 <dd><strong><code>AnalyserNode</code></strong> を利用すると、音声のリアルタイムに時間領域分析と周波数領域分析が行えます。これを利用すると、音声の可視化が行えます。</dd>
</dl>

<h3 id="オーディオチャンネルの分岐と合成">オーディオチャンネルの分岐と合成</h3>

<p>オーディオチャンネルを分岐したり合成したりするのにこれらのインターフェースを使います。</p>

<dl>
 <dt>{{domxref("ChannelSplitterNode")}}</dt>
 <dd>The <code><strong>ChannelSplitterNode</strong></code> はオーディオソースの複数のチャンネルを別々のモノラル出力へ分離します。</dd>
 <dt>{{domxref("ChannelMergerNode")}}</dt>
 <dd><code><strong>ChannelMergerNode</strong></code> は異なるモノラルの入力を、1 つの出力へとまとめます。それぞれの入力は、出力内のチャンネルとなります。</dd>
</dl>

<h3 id="立体音響">立体音響</h3>

<p>以下を利用すると、立体音響を実現できます。</p>

<dl>
 <dt>{{domxref("AudioListener")}}</dt>
 <dd><strong><code>AudioListener</code></strong><strong> </strong>は聴者の向きと位置を表します。</dd>
 <dt>{{domxref("PannerNode")}}</dt>
 <dd><strong><code>PannerNode</code></strong><strong> </strong>は {{domxref("AudioNode")}} の一種で、空間内での音の振る舞いを規定します。位置はカルテシアンの右手座標系で表され、速度ベクトルで動きを表します。向きはコーンの向きで表現します。</dd>
 <dt>{{domxref("StereoPannerNode")}}</dt>
 <dd><code><strong>StereoPannerNode</strong></code> インターフェイスは単純なステレオプランナーで、音声ストリームのパン(左右バランス)を調整できます。</dd>
</dl>

<h3 id="JavaScript_による音声処理">JavaScript による音声処理</h3>

<p>音声Workletを使うことで、JavaScriptや<a href="/ja/docs/WebAssembly">WebAssembly</a>を使って自作の{{domxref("AudioNode")}}を定義できます。音声Workletは {{domxref("Worklet")}} インターフェイスという軽量版 {{domxref("Worker")}} インターフェイスを実装しています。 Chrome 66 以降で既定で有効です。</p>

<dl>
 <dt>{{domxref("AudioWorklet")}} {{experimental_inline}}</dt>
 <dd><code>AudioWorklet</code> インターフェイスは {{domxref("BaseAudioContext.audioWorklet")}} を介して利用でき、新たなモジュールを音声Workletに追加できます。</dd>
 <dt>{{domxref("AudioWorkletNode")}} {{experimental_inline}}</dt>
 <dd><code>AudioWorkletNode</code> インターフェイスは {{domxref("AudioNode")}} の一種で、音声グラフに組み込んだり、対応する <code>AudioWorkletProcessor</code> にメッセージを送信できます。</dd>
 <dt>{{domxref("AudioWorkletProcessor")}} {{experimental_inline}}</dt>
 <dd><code>AudioWorkletProcessor</code> インターフェイスは <code>AudioWorkletGlobalScope</code> で実行する音声処理コードで、音声の生成・処理・分析を直接行ったり、対応する <code>AudioWorkletNode</code> にメッセージを送信できます。</dd>
 <dt>{{domxref("AudioWorkletGlobalScope")}} {{experimental_inline}}</dt>
 <dd><code>AudioWorkletGlobalScope</code> インターフェイスは <code>WorkletGlobalScope</code> から派生するオブジェクトで、音声処理スクリプトが実行されるワーカーコンテキストを表現します。WorkletスレッドでJavaScriptを用いて音声の生成・処理・分析を直接行えるよう設計されています。</dd>
</dl>

<h4 id="廃止_script_processor_nodes">廃止: script processor nodes</h4>

<p>音声Workletが定義されるよりも昔、 Web Audio API はJavaScriptを使用する音声処理に <code>ScriptProcessorNode</code>  を利用していました。しかしながら処理がメインスレッドで走るためにパフォーマンスが良くありませんでした。歴史的な理由から <code>ScriptProcessorNode</code>  は維持されていますが非推奨であり、将来の規格から取り除かれる予定です。</p>

<dl>
 <dt>{{domxref("ScriptProcessorNode")}} {{deprecated_inline}}</dt>
 <dd><strong><code>ScriptProcessorNode</code></strong><strong> </strong>を利用すると、JavaScript から音声データの生成、処理、分析を行えます。このノードは {{domxref("AudioNode")}} の一種で、入力と出力の二つのバッファとリンクしています。入力バッファに新しいデータがセットされる度に {{domxref("AudioProcessingEvent")}} インタフェースを実装したイベントが生起します。イベントハンドラは出力バッファにデータをセットして処理を終了します。</dd>
 <dt>{{event("audioprocess")}} (event) {{deprecated_inline}}</dt>
 <dd><code>audioprocess</code> イベントは {{domxref("ScriptProcessorNode")}} の処理が可能になった際に発火します。</dd>
 <dt>{{domxref("AudioProcessingEvent")}} {{deprecated_inline}}</dt>
 <dd><code>AudioProcessingEvent</code> は {{domxref("ScriptProcessorNode")}} の入力バッファが処理可能になったことを表すイベントです。</dd>
</dl>

<h3 id="オフライン_バックグラウンドでの処理">オフライン / バックグラウンドでの処理</h3>

<p>以下を利用すると、音声を出力することなく処理できます。</p>

<dl>
 <dt>{{domxref("OfflineAudioContext")}}</dt>
 <dd><strong><code>OfflineAudioContext</code></strong> は {{domxref("AudioContext")}} の一種で、{{domxref("AudioNode")}} を組み合わせて、音声処理を行うグラフを表現しています。通常の <code>AudioContext </code>と異なり<code>、</code><code>OfflineAudioContext</code> は音声を出力せず、バッファ内で高速に処理を行います。</dd>
 <dt>{{event("complete")}} (event)</dt>
 <dd><code>complete</code> イベントは {{domxref("OfflineAudioContext")}} の処理が終了した時に発火します。</dd>
 <dt>{{domxref("OfflineAudioCompletionEvent")}}</dt>
 <dd><code>OfflineAudioCompletionEvent</code> は {{domxref("OfflineAudioContext")}} の処理が終了したことを表します。{{event("complete")}} イベントは、これを実装しています。</dd>
</dl>

<h2 id="Example" name="Example">廃止されたインタフェース</h2>

<p>以下のものは、 Web Audio API の古い仕様には存在しましたが、現在は廃止され、別のものに置き換えられています。</p>

<dl>
 <dt>{{domxref("JavaScriptNode")}}</dt>
 <dd>JavaScript で音声を直接処理できます。廃止され、 {{domxref("ScriptProcessorNode")}} に置き換えられています。</dd>
 <dt>{{domxref("WaveTableNode")}}</dt>
 <dd>定期的な波形変換を行います。廃止され {{domxref("PeriodicWaveNode")}} に置き換えられています。</dd>
</dl>

<h2 id="Example" name="Example">例</h2>

<p>GitHubの <a href="https://github.com/mdn/webaudio-examples/">webaudio-example</a> に多数の例が掲載されています。</p>

<h2 id="仕様">仕様</h2>

<table class="standard-table">
 <tbody>
  <tr>
   <th scope="col">Specification</th>
   <th scope="col">Status</th>
   <th scope="col">Comment</th>
  </tr>
  <tr>
   <td>{{SpecName('Web Audio API')}}</td>
   <td>{{Spec2('Web Audio API')}}</td>
   <td></td>
  </tr>
 </tbody>
</table>

<h2 id="ブラウザ互換性">ブラウザ互換性</h2>

<div>
<h3 id="AudioContext"><code>AudioContext</code></h3>

<div>
<div class="hidden">The compatibility table on this page is generated from structured data. If you'd like to contribute to the data, please check out <a href="https://github.com/mdn/browser-compat-data">https://github.com/mdn/browser-compat-data</a> and send us a pull request.</div>

<p>{{Compat("api.AudioContext", 0)}}</p>
</div>
</div>

<h2 id="関連情報">関連情報</h2>

<h3 id="Tutorialsguides">Tutorials/guides</h3>

<ul>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Basic concepts behind Web Audio API</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques">Advanced techniques: creating sound, sequencing, timing, scheduling</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide">Autoplay guide for media and Web Audio APIs</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_IIR_filters">Using IIR filters</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API">Visualizations with Web Audio API</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialisation_basics">Web audio spatialisation basics</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode">Controlling multiple parameters with ConstantSourceNode</a></li>
 <li><a href="http://www.html5rocks.com/tutorials/webaudio/positional_audio/" title="http://www.html5rocks.com/tutorials/webaudio/positional_audio/">Mixing Positional Audio and WebGL</a></li>
 <li><a href="http://www.html5rocks.com/tutorials/webaudio/games/" title="http://www.html5rocks.com/tutorials/webaudio/games/">Developing Game Audio with the Web Audio API</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext" title="/en-US/docs/Web_Audio_API/Porting_webkitAudioContext_code_to_standards_based_AudioContext">Porting webkitAudioContext code to standards based AudioContext</a></li>
</ul>

<h3 id="Libraries">Libraries</h3>

<ul>
 <li><a href="https://github.com/bit101/tones">Tones</a>: a simple library for playing specific tones/notes using the Web Audio API.</li>
 <li><a href="https://tonejs.github.io/">Tone.js</a>: a framework for creating interactive music in the browser.</li>
 <li><a href="https://github.com/goldfire/howler.js/">howler.js</a>: a JS audio library that defaults to <a href="https://webaudio.github.io/web-audio-api/">Web Audio API</a> and falls back to <a href="http://www.whatwg.org/specs/web-apps/current-work/#the-audio-element">HTML5 Audio</a>, as well as providing other useful features.</li>
 <li><a href="https://github.com/mattlima/mooog">Mooog</a>: jQuery-style chaining of AudioNodes, mixer-style sends/returns, and more.</li>
 <li><a href="https://korilakkuma.github.io/XSound/">XSound</a>: Web Audio API Library for Synthesizer, Effects, Visualization, Recording ... etc</li>
 <li><a class="external external-icon" href="https://github.com/chrisjohndigital/OpenLang">OpenLang</a>: HTML5 video language lab web application using the Web Audio API to record and combine video and audio from different sources into a single file (<a class="external external-icon" href="https://github.com/chrisjohndigital/OpenLang">source on GitHub</a>)</li>
 <li><a href="https://ptsjs.org/">Pts.js</a>: Simplifies web audio visualization (<a href="https://ptsjs.org/guide/sound-0800">guide</a>)</li>
</ul>

<h3 id="Related_topics">Related topics</h3>

<ul>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/Media">Web media technologies</a></li>
 <li><a href="https://wiki.developer.mozilla.org/en-US/docs/Web/Media/Formats">Guide to media types and formats on the web</a></li>
</ul>