<p>{{APIRef("Web Audio API")}}</p>

<div class="summary">
<p><span class="seoSummary"><strong><code>AudioContext</code></strong>接口表示由音频模块连接而成的音频处理图，每个模块对应一个{{domxref("AudioNode")}}。<strong><code>AudioContext</code></strong>可以控制它所包含的节点的创建，以及音频处理、解码操作的执行。做任何事情之前都要先创建<strong><code>AudioContext</code></strong>对象，因为一切都发生在这个环境之中。</span></p>
</div>

<p><code>AudioContext</code> 可以是事件源（event target），所以也实现了{{domxref("EventTarget")}} 接口。</p>

<h2 id="属性">属性</h2>

<dl>
 <dt>{{domxref("AudioContext.currentTime")}} {{readonlyInline}}</dt>
 <dd>以双精度浮点型数字返回硬件调用的秒数，AudioContext一创建就从0开始走，无法停掉、暂停或者重置。</dd>
 <dt>{{domxref("AudioContext.destination")}} {{readonlyInline}}</dt>
 <dd>返回{{domxref("AudioDestinationNode")}}对象，表示当前audio context中所有节点的最终节点，一般表示音频渲染设备。</dd>
 <dt>{{domxref("AudioContext.listener")}} {{readonlyInline}}</dt>
 <dd>返回{{domxref("AudioListener")}}对象，用于3D音频空间化。</dd>
 <dt>{{domxref("AudioContext.sampleRate")}} {{readonlyInline}}</dt>
 <dd>返回用浮点数表示的采样率，也就是每秒的采样数，同一个AudioContext中的所有节点采样率相同，所以不支持采样率转换。</dd>
 <dt>{{domxref("AudioContext.state")}} {{readonlyInline}}</dt>
 <dd>返回<code>AudioContext当前状态</code>.</dd>
 <dt>{{domxref("AudioContext.mozAudioChannelType")}} {{ non-standard_inline() }} {{readonlyInline}}</dt>
 <dd>被用于返回一个 Firefox OS 设备上{{domxref("AudioContext")}} 将会播放的音频声道.</dd>
 <dt>
 <h3 id="事件处理程序">事件处理程序</h3>
 </dt>
 <dt>{{domxref("AudioContext.onstatechange")}}</dt>
 <dd>当一个 {{event("statechange")}} 事件类型被触发时响应的事件处理程序. <code>AudioContext的状态会因</code> ({{domxref("AudioContext.suspend")}}, {{domxref("AudioContext.resume")}}或者{{domxref("AudioContext.close")}})方法的运用而发生改变</dd>
</dl>

<h2 id="Methods">Methods</h2>

<p><em>可以实现 </em>{{domxref("EventTarget")}}接口的方法.</p>

<dl>
 <dt>{{domxref("AudioContext.close()")}}</dt>
 <dd>关闭一个音频环境, 释放任何正在使用系统资源的音频.</dd>
 <dt>{{domxref("AudioContext.createBuffer()")}}</dt>
 <dd>创建一个空的{{ domxref("AudioBuffer") }} 对象, 并且能够通过 {{ domxref("AudioBufferSourceNode") }}来进行数据填充和播放.</dd>
 <dt>{{domxref("AudioContext.createConstantSource()")}}</dt>
 <dd>创建一个{{domxref("ConstantSourceNode")}} 对象, 它持续输出一个连续的单声道，这些样本都会拥有一个相同的固定值。</dd>
 <dt>{{domxref("AudioContext.createBufferSource()")}}</dt>
 <dd>创建一个 {{domxref("AudioBufferSourceNode")}} 对象, 他可以通过{{ domxref("AudioBuffer") }}对象来播放和处理包含在内的音频数据. {{ domxref("AudioBuffer") }}可以通过{{domxref("AudioContext.createBuffer")}}方法创建或者使用{{domxref("AudioContext.decodeAudioData")}}方法解码音轨来创建。</dd>
 <dt>{{domxref("AudioContext.createMediaElementSource()")}}</dt>
 <dd>创建一个{{domxref("MediaElementAudioSourceNode")}}接口来关联{{domxref("HTMLMediaElement")}}. 这可以用来播放和处理来自{{HTMLElement("video")}}或{{HTMLElement("audio")}} 元素的音频.</dd>
 <dt>{{domxref("AudioContext.createMediaStreamSource()")}}</dt>
 <dd>创建一个{{domxref("MediaStreamAudioSourceNode")}}接口来关联可能来自本地计算机麦克风或其他来源的音频流{{domxref("MediaStream")}}.</dd>
 <dt>{{domxref("AudioContext.createMediaStreamDestination()")}}</dt>
 <dd>创建一个{{domxref("MediaStreamAudioDestinationNode")}}接口来关联可能储存在本地或已发送至其他计算机的{{domxref("MediaStream")}}音频.</dd>
 <dt>{{domxref("AudioContext.createScriptProcessor()")}}</dt>
 <dd>创建一个可以通过JavaScript直接处理音频的{{domxref("ScriptProcessorNode")}}.</dd>
 <dt>{{domxref("AudioContext.createStereoPanner()")}}</dt>
 <dd>创建一个使用立体声的音频源{{domxref("StereoPannerNode")}}.</dd>
 <dt>{{domxref("AudioContext.createAnalyser()")}}</dt>
 <dd>创建一个{{domxref("AnalyserNode")}}，它可以用来显示音频时间和频率的数据。</dd>
 <dt>{{domxref("AudioContext.createBiquadFilter()")}}</dt>
 <dd>创建一个{{domxref("BiquadFilterNode")}}，它代表代表一个双二阶滤波器，可以设置几种不同且常见滤波器类型：高通、低通、带通等。</dd>
 <dt>{{domxref("AudioContext.createChannelMerger()")}}</dt>
 <dd>创建一个{{domxref("ChannelMergerNode")}}，它被用于从多个音频流信道结合成一个单一的音频流。</dd>
 <dt>{{domxref("AudioContext.createChannelSplitter()")}}</dt>
 <dd>创建一个{{domxref("ChannelSplitterNode")}}，它用于访问的音频流的单独的通道并分别对他们进行处理。</dd>
 <dt>{{domxref("AudioContext.createConvolver()")}}</dt>
 <dd>创建一个{{domxref("ConvolverNode")}}，它可用于混合效果，比如说混响效果。</dd>
 <dt>{{domxref("AudioContext.createDelay()")}}</dt>
 <dd>创建一个{{domxref("DelayNode")}}，它可以通过一定量的延迟传入音频信号，它也被用做创建一个反馈回路。</dd>
 <dt>{{domxref("AudioContext.createDynamicsCompressor()")}}</dt>
 <dd>创建一个{{domxref("DynamicsCompressorNode")}}, 它可用于声学的音频信号的压缩。</dd>
 <dt>{{domxref("AudioContext.createGain()")}}</dt>
 <dd>创建一个{{domxref("GainNode")}},它可以控制音频的总音量。</dd>
 <dt>{{domxref("AudioContext.createIIRFilter()")}}</dt>
 <dd>创建一个{{domxref("IIRFilterNode")}}，它可以将一个二阶滤波器配置为多种不同的通用滤波器类型。</dd>
 <dt>{{domxref("AudioContext.createOscillator()")}}</dt>
 <dd>创建一个{{domxref("OscillatorNode")}}, 它表示一个周期性波形，基本上来说创造了一个音调.</dd>
 <dt>{{domxref("AudioContext.createPanner()")}}</dt>
 <dd>创建一个{{domxref("PannerNode")}}, 它为音源创建一个3D音源环境。</dd>
 <dt>{{domxref("AudioContext.createPeriodicWave()")}}</dt>
 <dd>创建一个{{domxref("PeriodicWave")}}, 创建一个用来定义 {{ domxref("OscillatorNode") }} 的周期波形。</dd>
 <dt>{{domxref("AudioContext.createWaveShaper()")}}</dt>
 <dd>创建一个 {{domxref("WaveShaperNode")}}, 它被用于创建非线性失真效果.</dd>
 <dt>{{domxref("AudioContext.createAudioWorker()")}}</dt>
 <dd>创建一个{{domxref("AudioWorkerNode")}}, 它可以通过使用worker来产生，处理，或直接分析音频. </dd>
 <dt>{{domxref("AudioContext.decodeAudioData()")}}</dt>
 <dd>从{{domxref("ArrayBuffer")}}对象中异步解码音频文件. 在此情况下,这个ArrayBuffer对象通常是通过使用 responseType为arraybuffer类型的{{domxref("XMLHttpRequest")}}方法来获取的. 该方法只能作用于完整的音频文件.</dd>
 <dt>{{domxref("AudioContext.resume()")}}</dt>
 <dd>重新启动一个已被暂停的音频环境</dd>
 <dt>{{domxref("AudioContext.suspend()")}}</dt>
 <dd>暂停音频内容的进度.暂时停止音频硬件访问和减少在过程中的CPU/电池使用.</dd>
</dl>

<h2 id="已被废弃的方法">已被废弃的方法</h2>

<dl>
 <dt>{{domxref("AudioContext.createJavaScriptNode()")}}</dt>
 <dd>创建一个 {{domxref("JavaScriptNode")}}, 用于javascript直接处理音频。 这个方法已经被{{domxref("AudioContext.createScriptProcessor()")}}替代并且废弃。</dd>
 <dt>{{domxref("AudioContext.createWaveTable()")}}</dt>
 <dd>创建一个 {{domxref("WaveTableNode")}}, 用于顶一个周期性波形。 这个方法已经被{{domxref("AudioContext.createPeriodicWave()")}}替代并且废弃。</dd>
</dl>

<h2 id="例子">例子</h2>

<p>简单声明：</p>

<pre class="brush: js">var audioCtx = new AudioContext;</pre>

<p>跨浏览器的方式：</p>

<pre class="brush: js">var audioCtx = new (window.AudioContext || window.webkitAudioContext)(); // declare new audio context
// Webkit/blink browser require a prefix, and it needs the window object specifically declared to work in Safari

var oscillatorNode = audioCtx.createOscillator();
var gainNode = audioCtx.createGain();
var finish = audioCtx.destination;
// etc.</pre>

<h2 id="规范">规范</h2>

<table class="standard-table">
 <tbody>
  <tr>
   <th scope="col">规范</th>
   <th scope="col">状态</th>
   <th scope="col">注释</th>
  </tr>
  <tr>
   <td>{{SpecName('Web Audio API', '#AudioContext-section', 'AudioContext')}}</td>
   <td>{{Spec2('Web Audio API')}}</td>
   <td> </td>
  </tr>
 </tbody>
</table>

<h2 id="浏览器兼容性">浏览器兼容性</h2>

<div>{{CompatibilityTable}}</div>

<div id="compat-desktop">
<table class="compat-table">
 <tbody>
  <tr>
   <th>Feature</th>
   <th>Chrome</th>
   <th>Firefox (Gecko)</th>
   <th>Internet Explorer</th>
   <th>Opera</th>
   <th>Safari (WebKit)</th>
  </tr>
  <tr>
   <td>Basic support</td>
   <td>{{CompatChrome(10.0)}}{{property_prefix("webkit")}}</td>
   <td>{{CompatGeckoDesktop(25.0)}} </td>
   <td>{{CompatNo}}</td>
   <td>15.0{{property_prefix("webkit")}}</td>
   <td>6.0{{property_prefix("webkit")}}</td>
  </tr>
 </tbody>
</table>
</div>

<div id="compat-mobile">
<table class="compat-table">
 <tbody>
  <tr>
   <th>Feature</th>
   <th>Android</th>
   <th>Firefox for Andriod</th>
   <th>IE Mobile</th>
   <th>Opera Mobile</th>
   <th>Safari Mobile</th>
   <th>Chrome for Andriod</th>
  </tr>
  <tr>
   <td>Basic support</td>
   <td>{{CompatUnknown}}</td>
   <td>26.0</td>
   <td>{{CompatUnknown}}</td>
   <td>{{CompatUnknown}}</td>
   <td>{{CompatUnknown}}</td>
   <td>33.0</td>
  </tr>
 </tbody>
</table>
</div>

<h2 id="另见">另见</h2>

<ul>
 <li><a href="/en-US/docs/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</a></li>
</ul>